{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the test environment for `pipeline.py`.\n",
    "\n",
    "It runs Python 3.11.7 on anaconda3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "'''Data pipeline to pull data from local csv and load data into the DF database.'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "import psycopg2 as psql # type: ignore\n",
    "import warnings\n",
    "\n",
    "class EnvSecrets:\n",
    "    def __init__(self) -> None:\n",
    "        '''Fetch all sensitive data and load into variables for later use.'''\n",
    "        load_dotenv()\n",
    "        self.db_name = os.getenv('DATABASE_NAME')\n",
    "        self.db_host = os.getenv('DATABASE_HOST')\n",
    "        self.db_port = int(os.getenv('DATABASE_PORT'))\n",
    "        self.sql_user = os.getenv('SQL_USERNAME')\n",
    "        self.sql_pass = os.getenv('SQL_PASSWORD')\n",
    "        self.schema = os.getenv('SQL_SCHEMA')\n",
    "\n",
    "class DatabaseConnection:\n",
    "    def __init__(self, secret:EnvSecrets) -> None:\n",
    "        '''Open database connection.'''\n",
    "        try:\n",
    "            self.connection = psql.connect(\n",
    "                database = secret.db_name\n",
    "                , host = secret.db_host\n",
    "                , port = secret.db_port\n",
    "                , user = secret.sql_user\n",
    "                , password = secret.sql_pass\n",
    "            )\n",
    "            self.cursor = self.connection.cursor()\n",
    "            self.valid = True\n",
    "        except:\n",
    "            '''Possible expansion for email sending, notifying database failure or invalid credentials.'''\n",
    "            self.valid = False\n",
    "    def close(self) -> None:\n",
    "        '''Close database connection.'''\n",
    "        try:\n",
    "            self.connection.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, secret, csv_source) -> None:\n",
    "        self.new_data = pd.DataFrame()\n",
    "        self.secret = secret\n",
    "        self.csv = csv_source\n",
    "    def extract(self) -> None:\n",
    "        self.new_data = pd.read_csv(self.csv)\n",
    "    def transform(self) -> None:\n",
    "        self.new_data = self.new_data[(self.new_data['type']=='model')]\n",
    "        for column_name in self.new_data.copy().columns:\n",
    "            if len(self.new_data[column_name].unique()) <= 1:\n",
    "                self.new_data.drop(columns=column_name, inplace=True)\n",
    "        self.new_data['model_pk'] = range(1, 1+len(self.new_data)) # primary key for model\n",
    "        self.new_data['created_date'] = pd.to_datetime(self.new_data['created_date'], errors='coerce')\n",
    "        _temp = self.new_data['modality'].apply(repackage_modality).apply(pd.Series)\n",
    "        self.new_data[['input_modality', 'output_modality']] = _temp\n",
    "        self.new_data.drop(columns='modality', inplace=True)\n",
    "        self.new_data[['dependencies']] = self.new_data[['dependencies']].copy().map(\n",
    "            lambda raw: [s.strip(' ').strip('\\'') for s in str(raw)[1:-1].split(',')]\n",
    "        )\n",
    "        self.new_data = self.new_data.explode('input_modality')\n",
    "        self.new_data = self.new_data.explode('output_modality')\n",
    "        self.new_data = self.new_data.explode('dependencies')\n",
    "        self.new_data['table_pk'] = range(1, 1+len(self.new_data))\n",
    "    def load(self, database) -> None:\n",
    "        table_name = 'hackathon_july24_test'\n",
    "        primary_key = 'table_pk'\n",
    "        self.backup(primary_key)\n",
    "        varchar_length = 10000\n",
    "        create_table_sql = f'''\n",
    "CREATE TABLE IF NOT EXISTS {self.secret.schema}.{table_name} (\n",
    "    {primary_key} int PRIMARY KEY\n",
    "    , model_pk int\n",
    "    , name varchar({varchar_length})\n",
    "    , organization varchar({varchar_length})\n",
    "    , description varchar({varchar_length})\n",
    "    , created_date date\n",
    "    , url varchar({varchar_length})\n",
    "    , size varchar({varchar_length})\n",
    "    , analysis varchar({varchar_length})\n",
    "    , dependencies varchar({varchar_length})\n",
    "    , quality_control varchar({varchar_length})\n",
    "    , access varchar({varchar_length})\n",
    "    , license varchar({varchar_length})\n",
    "    , intended_uses varchar({varchar_length})\n",
    "    , prohibited_uses varchar({varchar_length})\n",
    "    , monitoring varchar({varchar_length})\n",
    "    , feedback varchar({varchar_length})\n",
    "    , model_card varchar({varchar_length})\n",
    "    , training_emissions varchar({varchar_length})\n",
    "    , training_time varchar({varchar_length})\n",
    "    , training_hardware varchar({varchar_length})\n",
    "    , input_modality varchar({varchar_length})\n",
    "    , output_modality varchar({varchar_length})\n",
    ");'''\n",
    "        database.cursor.execute(create_table_sql)\n",
    "        self.new_data = self.new_data.replace({np.nan: None})\n",
    "        self.new_data['created_date'] = self.new_data['created_date'].replace({pd.NaT: None})\n",
    "        query = f'SELECT {primary_key} FROM {self.secret.schema}.{table_name}'\n",
    "        existing_data = pd.read_sql(query, database.connection)\n",
    "        existing_set = set(existing_data[primary_key])\n",
    "        non_duplicate_data = self.new_data[~self.new_data[primary_key].isin(existing_set)]\n",
    "        if not non_duplicate_data.empty:\n",
    "            tuples = [tuple(x) for x in non_duplicate_data.to_numpy()]\n",
    "            columns = ','.join(non_duplicate_data.columns)\n",
    "            values = ','.join(['%s'] * len(non_duplicate_data.columns))\n",
    "            insert_query = f'INSERT INTO {self.secret.schema}.{table_name} ({columns}) VALUES ({values})'\n",
    "            database.cursor.executemany(insert_query, tuples)\n",
    "        database.connection.commit()\n",
    "    def backup(self, primary_key) -> None:\n",
    "        '''Adding to a back up csv file.'''\n",
    "        file_name = '../data/processed.csv'\n",
    "        try:\n",
    "            existing_data = pd.read_csv(file_name)\n",
    "            for column_name in existing_data.columns:\n",
    "                if 'Unnamed' in column_name:\n",
    "                    existing_data.drop(columns=column_name, inplace=True)\n",
    "            incoming_data = pd.concat([existing_data, self.new_data], ignore_index=True)\n",
    "            incoming_data.set_index(primary_key, inplace=True)\n",
    "            incoming_data.drop_duplicates(keep='first', inplace=True)\n",
    "            incoming_data.sort_index(inplace=True)\n",
    "            incoming_data.reset_index(inplace=True)\n",
    "            incoming_data.to_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            self.new_data.to_csv(file_name)\n",
    "\n",
    "def repackage_modality(raw:str) -> tuple[list[str]]:\n",
    "    raw = str(raw)\n",
    "    semicolon_count = raw.count(';')\n",
    "    assert semicolon_count <= 1, 'LLM modality invalid.'\n",
    "    if semicolon_count == 0:\n",
    "        raw = raw + ';' + raw\n",
    "    modal_input_str, modal_output_str = raw.split(';')\n",
    "    modal_inputs = [s.strip() for s in modal_input_str.split(',')]\n",
    "    modal_outputs = [s.strip() for s in modal_output_str.split(',')]\n",
    "    return modal_inputs, modal_outputs\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    warnings.filterwarnings('ignore', message='.*pandas only supports SQLAlchemy connectable.*')\n",
    "    csv_filepath = '../data/assets.csv'\n",
    "    sensitive_data = EnvSecrets()\n",
    "    df_database = DatabaseConnection(sensitive_data)\n",
    "    database_seeding = DataPipeline(sensitive_data, csv_filepath)\n",
    "    database_seeding.extract()\n",
    "    database_seeding.transform()\n",
    "    try:\n",
    "        database_seeding.load(df_database)\n",
    "        print('Table created successfully!')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        df_database.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
