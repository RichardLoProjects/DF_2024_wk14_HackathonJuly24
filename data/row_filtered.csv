,type,name,organization,description,created_date,url,size,analysis,dependencies,quality_control,access,license,intended_uses,prohibited_uses,monitoring,feedback,model_card,training_emissions,training_time,training_hardware,adaptation,output_space,terms_of_service,monthly_active_users,user_distribution,failures,model_id,input_modality,output_modality,table_pk
127,model,GPT-J,EleutherAI,GPT-J is an open-source autoregressive language model.,2021-06-04,https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/,6B parameters (dense),,The Pile,,open,Apache 2.0,,,,,,,,TRC (Unspecified # of TPU v3-8s),,,,,,,65,text,text,128
403,model,Gemini,Google,"As of release, Gemini is Google's most capable and flexible AI model, proficient in multimodal domains.",2023-12-06,https://deepmind.google/technologies/gemini/#introduction,unknown,"Evaluated on standard general, reasoning, math, coding, and multimodal benchmarks with results that surpass GPT-4 on almost all.",,,closed,unknown,"general use large language model that can be used for language, reasoning, and code tasks.",becoming part of a general-purpose service or product or use within specific downstream applications without prior assessment,Google internal monitoring,,,unknown,unknown,unknown,,,,,,,172,text,image,404
404,model,Gemini,Google,"As of release, Gemini is Google's most capable and flexible AI model, proficient in multimodal domains.",2023-12-06,https://deepmind.google/technologies/gemini/#introduction,unknown,"Evaluated on standard general, reasoning, math, coding, and multimodal benchmarks with results that surpass GPT-4 on almost all.",,,closed,unknown,"general use large language model that can be used for language, reasoning, and code tasks.",becoming part of a general-purpose service or product or use within specific downstream applications without prior assessment,Google internal monitoring,,,unknown,unknown,unknown,,,,,,,172,text,text,405
405,model,Gemini,Google,"As of release, Gemini is Google's most capable and flexible AI model, proficient in multimodal domains.",2023-12-06,https://deepmind.google/technologies/gemini/#introduction,unknown,"Evaluated on standard general, reasoning, math, coding, and multimodal benchmarks with results that surpass GPT-4 on almost all.",,,closed,unknown,"general use large language model that can be used for language, reasoning, and code tasks.",becoming part of a general-purpose service or product or use within specific downstream applications without prior assessment,Google internal monitoring,,,unknown,unknown,unknown,,,,,,,172,text,video,406
475,model,GPT-4,OpenAI,,2023-03-14,https://arxiv.org/abs/2303.08774,unknown,,,,limited,unknown,,,,,,,,,,,,,,,215,image,image,476
476,model,GPT-4,OpenAI,,2023-03-14,https://arxiv.org/abs/2303.08774,unknown,,,,limited,unknown,,,,,,,,,,,,,,,215,image,text,477
477,model,GPT-4,OpenAI,,2023-03-14,https://arxiv.org/abs/2303.08774,unknown,,,,limited,unknown,,,,,,,,,,,,,,,215,text,image,478
478,model,GPT-4,OpenAI,,2023-03-14,https://arxiv.org/abs/2303.08774,unknown,,,,limited,unknown,,,,,,,,,,,,,,,215,text,text,479
487,model,BloombergGPT,Bloomberg,BloombergGPT is a 50 billion parameter large language model that is specifically trained on a wide range of financial data to support a diverse set of natural language processing tasks within the financial industry.,2023-03-30,https://arxiv.org/abs/2303.17564,50B parameters (dense),"Authors evaluate the performance of BloombergGPT on two broad categories of tasks, finance-specific and general purpose, on several standard benchmarks. They compare BloombergGPT to the three closest models: GPT-NeoX, OPT-66B and BLOOM-176B. They also report results from the original GPT-3 whenever externally available. They conclude ""We achieve strong results on general LLM benchmarks and outperform comparable models on financial tasks. We attribute this, in decreasing order of impact, to 1. a well-curated internal dataset, 2. our unique choice in tokenizer, and 3. an up-to-date architecture.""
",FinPile,"Authors state the following:
- ""To provide natural language applications to the financial community, we
  have developed a rigorous risk and testing assessment process. This process
  includes careful annotation guidelines Tseng et al. (2020), pre-launch review
  at multiple levels by the central risk and compliance organizations, and
  by the product leaders (e.g., the newsroom) as applicable, and post-launch
  monitoring. Moreover, we conduct our research, development, and deployment
  of NLP and AI systems in accordance with all applicable regulations.""
- ""Similarly, toxicity and bias are areas where, as a company, we take extraordinary
  care with any content we produce, whether from humans or machines. Since
  the measurement of toxicity and bias in our model depends on its application
  areas, quantifying the potential for the generation of harmful language
  remains an open question. We are particularly interested in studying whether
  FinPile, which is cleaner and contains fewer examples of overtly biased
  or toxic language (e.g., Press Releases), reduces the proclivity of the
  model to generate inappropriate content.""
",closed,unknown,"""This model will assist Bloomberg in improving existing financial NLP tasks, such as sentiment analysis, named entity recognition, news classification, and question answering, among others. Furthermore, BloombergGPT will unlock new opportunities for marshalling the vast quantities of data available on the Bloomberg Terminal to better help the firm's customers, while bringing the full potential of AI to the financial domain.""
",,,,,unknown,53 days,64 Amazon EC2 p4d.24xlarge instances each with 8 NVIDIA 40GB A100 GPUs (i.e. total 512 A100 GPUs),,,,,,,221,text,text,488
488,model,BloombergGPT,Bloomberg,BloombergGPT is a 50 billion parameter large language model that is specifically trained on a wide range of financial data to support a diverse set of natural language processing tasks within the financial industry.,2023-03-30,https://arxiv.org/abs/2303.17564,50B parameters (dense),"Authors evaluate the performance of BloombergGPT on two broad categories of tasks, finance-specific and general purpose, on several standard benchmarks. They compare BloombergGPT to the three closest models: GPT-NeoX, OPT-66B and BLOOM-176B. They also report results from the original GPT-3 whenever externally available. They conclude ""We achieve strong results on general LLM benchmarks and outperform comparable models on financial tasks. We attribute this, in decreasing order of impact, to 1. a well-curated internal dataset, 2. our unique choice in tokenizer, and 3. an up-to-date architecture.""
",The Pile,"Authors state the following:
- ""To provide natural language applications to the financial community, we
  have developed a rigorous risk and testing assessment process. This process
  includes careful annotation guidelines Tseng et al. (2020), pre-launch review
  at multiple levels by the central risk and compliance organizations, and
  by the product leaders (e.g., the newsroom) as applicable, and post-launch
  monitoring. Moreover, we conduct our research, development, and deployment
  of NLP and AI systems in accordance with all applicable regulations.""
- ""Similarly, toxicity and bias are areas where, as a company, we take extraordinary
  care with any content we produce, whether from humans or machines. Since
  the measurement of toxicity and bias in our model depends on its application
  areas, quantifying the potential for the generation of harmful language
  remains an open question. We are particularly interested in studying whether
  FinPile, which is cleaner and contains fewer examples of overtly biased
  or toxic language (e.g., Press Releases), reduces the proclivity of the
  model to generate inappropriate content.""
",closed,unknown,"""This model will assist Bloomberg in improving existing financial NLP tasks, such as sentiment analysis, named entity recognition, news classification, and question answering, among others. Furthermore, BloombergGPT will unlock new opportunities for marshalling the vast quantities of data available on the Bloomberg Terminal to better help the firm's customers, while bringing the full potential of AI to the financial domain.""
",,,,,unknown,53 days,64 Amazon EC2 p4d.24xlarge instances each with 8 NVIDIA 40GB A100 GPUs (i.e. total 512 A100 GPUs),,,,,,,221,text,text,489
489,model,BloombergGPT,Bloomberg,BloombergGPT is a 50 billion parameter large language model that is specifically trained on a wide range of financial data to support a diverse set of natural language processing tasks within the financial industry.,2023-03-30,https://arxiv.org/abs/2303.17564,50B parameters (dense),"Authors evaluate the performance of BloombergGPT on two broad categories of tasks, finance-specific and general purpose, on several standard benchmarks. They compare BloombergGPT to the three closest models: GPT-NeoX, OPT-66B and BLOOM-176B. They also report results from the original GPT-3 whenever externally available. They conclude ""We achieve strong results on general LLM benchmarks and outperform comparable models on financial tasks. We attribute this, in decreasing order of impact, to 1. a well-curated internal dataset, 2. our unique choice in tokenizer, and 3. an up-to-date architecture.""
",C4,"Authors state the following:
- ""To provide natural language applications to the financial community, we
  have developed a rigorous risk and testing assessment process. This process
  includes careful annotation guidelines Tseng et al. (2020), pre-launch review
  at multiple levels by the central risk and compliance organizations, and
  by the product leaders (e.g., the newsroom) as applicable, and post-launch
  monitoring. Moreover, we conduct our research, development, and deployment
  of NLP and AI systems in accordance with all applicable regulations.""
- ""Similarly, toxicity and bias are areas where, as a company, we take extraordinary
  care with any content we produce, whether from humans or machines. Since
  the measurement of toxicity and bias in our model depends on its application
  areas, quantifying the potential for the generation of harmful language
  remains an open question. We are particularly interested in studying whether
  FinPile, which is cleaner and contains fewer examples of overtly biased
  or toxic language (e.g., Press Releases), reduces the proclivity of the
  model to generate inappropriate content.""
",closed,unknown,"""This model will assist Bloomberg in improving existing financial NLP tasks, such as sentiment analysis, named entity recognition, news classification, and question answering, among others. Furthermore, BloombergGPT will unlock new opportunities for marshalling the vast quantities of data available on the Bloomberg Terminal to better help the firm's customers, while bringing the full potential of AI to the financial domain.""
",,,,,unknown,53 days,64 Amazon EC2 p4d.24xlarge instances each with 8 NVIDIA 40GB A100 GPUs (i.e. total 512 A100 GPUs),,,,,,,221,text,text,490
490,model,BloombergGPT,Bloomberg,BloombergGPT is a 50 billion parameter large language model that is specifically trained on a wide range of financial data to support a diverse set of natural language processing tasks within the financial industry.,2023-03-30,https://arxiv.org/abs/2303.17564,50B parameters (dense),"Authors evaluate the performance of BloombergGPT on two broad categories of tasks, finance-specific and general purpose, on several standard benchmarks. They compare BloombergGPT to the three closest models: GPT-NeoX, OPT-66B and BLOOM-176B. They also report results from the original GPT-3 whenever externally available. They conclude ""We achieve strong results on general LLM benchmarks and outperform comparable models on financial tasks. We attribute this, in decreasing order of impact, to 1. a well-curated internal dataset, 2. our unique choice in tokenizer, and 3. an up-to-date architecture.""
",Wikipedia,"Authors state the following:
- ""To provide natural language applications to the financial community, we
  have developed a rigorous risk and testing assessment process. This process
  includes careful annotation guidelines Tseng et al. (2020), pre-launch review
  at multiple levels by the central risk and compliance organizations, and
  by the product leaders (e.g., the newsroom) as applicable, and post-launch
  monitoring. Moreover, we conduct our research, development, and deployment
  of NLP and AI systems in accordance with all applicable regulations.""
- ""Similarly, toxicity and bias are areas where, as a company, we take extraordinary
  care with any content we produce, whether from humans or machines. Since
  the measurement of toxicity and bias in our model depends on its application
  areas, quantifying the potential for the generation of harmful language
  remains an open question. We are particularly interested in studying whether
  FinPile, which is cleaner and contains fewer examples of overtly biased
  or toxic language (e.g., Press Releases), reduces the proclivity of the
  model to generate inappropriate content.""
",closed,unknown,"""This model will assist Bloomberg in improving existing financial NLP tasks, such as sentiment analysis, named entity recognition, news classification, and question answering, among others. Furthermore, BloombergGPT will unlock new opportunities for marshalling the vast quantities of data available on the Bloomberg Terminal to better help the firm's customers, while bringing the full potential of AI to the financial domain.""
",,,,,unknown,53 days,64 Amazon EC2 p4d.24xlarge instances each with 8 NVIDIA 40GB A100 GPUs (i.e. total 512 A100 GPUs),,,,,,,221,text,text,491
755,model,Llama 3,Meta,Llama 3 is the third generation of Meta AI's open-source large language model. It comes with pretrained and instruction-fine-tuned language models with 8B and 70B parameters that can support a broad range of use cases.,2024-04-18,https://llama.meta.com/llama3/,70B parameters,"The models were evaluated based on their performance on standard benchmarks and real-world scenarios. These evaluations were performed using a high-quality human evaluation set containing 1,800 prompts covering multiple use cases. The models also went through red-teaming for safety, where human experts and automated methods were used to generate adversarial prompts to test for problematic responses.",,"Extensive internal and external testing for safety, and design of new trust and safety tools.",open,Llama 3,"Llama 3 is intended for a broad range of use cases, including AI assistance, content creation, learning, and analysis.",unknown,Extensive internal and external performance evaluation and red-teaming approach for safety testing.,"Feedback is encouraged from users to improve the model, but the feedback mechanism is not explicitly described.",https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md,unknown,unknown,2 custom-built Meta 24K GPU clusters,,,,,,,312,text,text,756
803,model,EXAONE 2.0,LG AI Research,EXAONE 2.0 is a multimodal artificial intelligence that can be used to help develop new materials and medicines.,2023-07-19,https://www.lgresearch.ai/exaone,unknown,,,,closed,unknown,,,,,,unknown,unknown,unknown,,,,,,,337,image,image,804
804,model,EXAONE 2.0,LG AI Research,EXAONE 2.0 is a multimodal artificial intelligence that can be used to help develop new materials and medicines.,2023-07-19,https://www.lgresearch.ai/exaone,unknown,,,,closed,unknown,,,,,,unknown,unknown,unknown,,,,,,,337,image,text,805
805,model,EXAONE 2.0,LG AI Research,EXAONE 2.0 is a multimodal artificial intelligence that can be used to help develop new materials and medicines.,2023-07-19,https://www.lgresearch.ai/exaone,unknown,,,,closed,unknown,,,,,,unknown,unknown,unknown,,,,,,,337,text,image,806
806,model,EXAONE 2.0,LG AI Research,EXAONE 2.0 is a multimodal artificial intelligence that can be used to help develop new materials and medicines.,2023-07-19,https://www.lgresearch.ai/exaone,unknown,,,,closed,unknown,,,,,,unknown,unknown,unknown,,,,,,,337,text,text,807
