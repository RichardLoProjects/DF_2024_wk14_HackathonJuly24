{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514d37d-2515-4d1a-bff5-aac00b0a0cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bde6b4e-da1f-4f49-b367-4dab205dd65a",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">\n",
    "LLM Matrix Data Dictionary\n",
    "</h1> \n",
    "<h5 style=\"text-align:center;\">  What is each column and how was it caluculated? </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4523f-92bd-4eca-b6c8-1cfc1f1ce1bc",
   "metadata": {},
   "source": [
    "| Section | Column Name | Description |\n",
    "|---------|-------------|-------------|\n",
    "|**Model Info** | \n",
    "| | **model** | The name of the model |\n",
    "|    |organisation | The organisation that created the model |\n",
    "|    |specialisation | What is the models specialisation, e.g. Finance, Medicine, General Purpose Model  |\n",
    "|    |created_at | The release date of the model |\n",
    "|**Bussiness Readiness** |\n",
    "| Credibility | cred_track_record | Looking at the organisations reputation, and any other projects they have done and how successful they've been <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| ^| cred_endorsements | Well known/ Prominent people or business have used it and said positive things about the model <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_  |\n",
    "| ^ | cred_recognition | Have the model recieved any awards? Are there anny mentions of the model in well known/ popular AI or tech spaces <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| ^ | _cred_score_ | Takes the average of `cred_track_record`, `cred_endorsemnets`, `cred_recognition` <br> <br> _Score between 0 - 4   (0 = bad credability ; 4 = good credability)_ |\n",
    "| Harmfulness | harm_incidents | Are there any reports or mentions of any incidents or mistakes made by the model? <br> <br> _Score between 0 - 4   (0 = bad - alot of incidents ; 4 = good - no incidents)_ |\n",
    "| ^ | harm_safeguards | What measures have the organisation taken to improve the outputs of the model so that less mistakes are made? <br> <br>  _Score between 0 - 4   (0 = bad - no safeguards ; 4 = good - lots of safeguards)_ |\n",
    "| ^ | _harm_score_ | Takes the average of `harm_incidents`, `harm_safeguards` <br> <br> _Score between 0 - 4   (0 = very harmful ; 4 = not harmful)_ |\n",
    "| Accuracy | accuracy_perc | An average accuracy, as a percentage, found from the internet. <br> This was dependant on the specialisation of the model: if the model is a 'General Purpose Model' an average accross many different tasks were taken, if the model is specialised in a field, e.g. Finance, we took the accuracy related to Financial Tasks |\n",
    "| ^ | accuracy_estimate | |\n",
    "| ^ | _accuracy_score_ | |\n",
    "| Benchmark | GLUE | |\n",
    "| ^ | SUPERGLUE | |\n",
    "| ^ | SQuAD | |\n",
    "| ^ | HELM_accuracy | |\n",
    "| ^ | HELM_efficiency | |\n",
    "| ^ | hugging_face_open_llm | |\n",
    "| ^ | vellum | |\n",
    "| ^ | other | |\n",
    "| ^ | estimated | |\n",
    "| ^ | _bench_score_ | |\n",
    "|  | **x_score** | |\n",
    "|**Pecieved Bussiness Value** |\n",
    "| Capabalities | _capabailities_ | <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| Success Stories | _success stories_ | <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| Popularity | pop_activity | <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| ^ | pop_growth_rate | <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| ^ | pop_variety | <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| ^ | _pop_score_ | <br> <br> _Score between 0 - 4   (0 = bad ; 4 = good)_ |\n",
    "| | **y_score** | |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
